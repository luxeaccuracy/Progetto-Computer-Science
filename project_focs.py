# -*- coding: utf-8 -*-
"""Project_FoCS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Gp7p7MF5EBDWSGXuEwptK8pdeVIBk27C
"""

import numpy as np
import pandas as pd

CA_videos = pd.read_csv("videos/CAvideos.csv")
DE_videos = pd.read_csv("videos/DEvideos.csv")
FR_videos = pd.read_csv("videos/FRvideos.csv")
IN_videos = pd.read_csv("videos/INvideos.csv")
US_videos = pd.read_csv("videos/USvideos.csv")
GB_videos = pd.read_csv("videos/GBvideos.csv")
JP_videos = pd.read_csv("videos/JPvideos.csv", encoding='utf-8', encoding_errors= 'replace') #Errore nella codifcia del file originale gestito rimpiazzando i caratteri
MX_videos = pd.read_csv("videos/MXvideos.csv", encoding='utf-8', encoding_errors= 'replace') #non codificabili con altri di rimpiazzo
RU_videos = pd.read_csv("videos/RUvideos.csv", encoding='utf-8', encoding_errors= 'replace')
KR_videos = pd.read_csv("videos/KRvideos.csv", encoding='utf-8', encoding_errors= 'replace')

"""1- Create a single dataframe with the concatenation of all input csv files, adding a column called country"""

CA_videos['country']='CA'
DE_videos['country']='DE'
FR_videos['country']='FR'
GB_videos['country']='GB'
IN_videos['country']='IN'
JP_videos['country']='JP'
KR_videos['country']='KR'
MX_videos['country']='MX'
RU_videos['country']='RU'
US_videos['country']='US'

country_list= (CA_videos, DE_videos, FR_videos, GB_videos, IN_videos, JP_videos, KR_videos, MX_videos, RU_videos, US_videos)
union = pd.concat(country_list, ignore_index=True)
union

"""2- Extract all videos that have no tag"""

no_tags = union[union["tags"] == "[none]"]
no_tags

"""3- For each channel, determine the total number of views"""

tot_views = union.groupby("channel_title")[["views"]].sum()
tot_views

"""4- Save all rows with disabled comments and disabled ratings, or that have video_error_or_removed in a new dataframe called excluded, and remove those rows from the original dataframe"""

excluded = union[(union["comments_disabled"] == True) & (union["ratings_disabled"] == True) | (union["video_error_or_removed"] == True)]
#Seleziona i video con commenti e valutazioni disabilitati oppure con errori/rimossi
union = union.drop(excluded.index)
union

"""5- Add a like_ratio column storing the ratio between the number of likes and of dislikes"""

union["like_ratio"] = union["likes"]/union["dislikes"]

"""6- Cluster the publish time into 10-minute intervals (e.g. from 02:20 to 02:30)"""

union["publish_time"] = pd.to_datetime(union["publish_time"])
union["range_10min"] = union["publish_time"].dt.floor("10min").dt.time #dt.floor dt=data e ora, floor=arrotonda per difetto
union.head()

"""7- For each interval, determine the number of videos, average number of likes and of dislikes."""

union_tot= union.groupby("range_10min").agg({"video_id": "count", "likes": "mean", "dislikes": "mean"})
union_tot

"""8- For each tag, determine the number of videos"""

no_tags2 = union[union["tags"] == "[none]"]
union_notags = union.drop(no_tags2.index)
tags = union_notags["tags"].str.replace('"', '').str.split("|")
tags_list= tags.explode()
tot_tags = tags_list.value_counts()
tot_tags

"""9- Find the tags with the largest number of videos"""

tot_tags.head(10)

"""10- For each (tag, country) pair, compute average ratio likes/dislikes"""

union_tags = (union_notags.assign(tags = union_notags["tags"].str.replace('"','').str.split(r"[|/]")).explode("tags").reset_index(drop=True))
#Pulisce i tag, li divide in singoli elementi e crea una riga per ciascun tag, resettando lâ€™indice.
union_tags.groupby(['country','tags'])['like_ratio'].mean()

"""11- For each (trending_date, country) pair, the video with the largest number of views"""

id_views = union_tags.groupby(['trending_date','country'])['views'].idxmax()
id_views = union_tags.loc[id_views]
id_views[['trending_date','country','title']]

"""12- Divide trending_date into three columns: year, month, day"""

union['day'] = union['publish_time'].dt.day
union['month'] = union['publish_time'].dt.month
union['year'] = union['publish_time'].dt.year

"""13- For each (month, country) pair, the video with the largest number of views"""

id_views_date = union.groupby(['month','country'])['views'].idxmax()
id_views_date = union.loc[id_views_date]
id_views_date[['month','country','title']]

"""14- Read all json files with the video categories"""

CA_category = pd.read_json("json/CA_category_id.json")
DE_category = pd.read_json("json/DE_category_id.json")
FR_category = pd.read_json("json/FR_category_id.json")
IN_category = pd.read_json("json/IN_category_id.json")
US_category = pd.read_json("json/US_category_id.json")
GB_category = pd.read_json("json/GB_category_id.json")
JP_category = pd.read_json("json/JP_category_id.json")
MX_category = pd.read_json("json/MX_category_id.json")
RU_category = pd.read_json("json/RU_category_id.json")
KR_category = pd.read_json("json/KR_category_id.json")

category_list = (CA_category, DE_category, FR_category, GB_category, IN_category, JP_category, KR_category, MX_category, RU_category, US_category)
category_union = pd.concat(category_list, ignore_index=True)
category_union_normalized = pd.json_normalize(category_union['items']) # Appiattisce la colonna "items" di tutti i file uniti
category_union_normalized = category_union_normalized.rename(columns={'snippet.title': 'title', 'snippet.assignable': 'assignable'})

"""15- For each country, determine how many videos have a category that is not assignable."""

category_union_normalized['id'] = category_union_normalized['id'].astype(int)
union_joined = pd.merge(union, category_union_normalized, left_on = 'category_id', right_on = 'id', how='outer')
union_joined_false = union_joined[union_joined['assignable'] == False]
union_joined_false.groupby('country')['video_id'].count()

